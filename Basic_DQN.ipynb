{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic_DQN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4QY4k3ExeVp"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np \n",
        "import matplotlib\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Utils"
      ],
      "metadata": {
        "id": "CKcrrMdDyAF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "resize = T.Compose([T.ToPILImage(),\n",
        "                    T.Resize(40, interpolation=Image.CUBIC),\n",
        "                    T.ToTensor()])\n",
        "\n",
        "def get_cart_location(env, screen_width):\n",
        "    world_width = env.x_threshold * 2\n",
        "    scale = screen_width / world_width\n",
        "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
        "\n",
        "def get_screen(env):\n",
        "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
        "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
        "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
        "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
        "    _, screen_height, screen_width = screen.shape\n",
        "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
        "    view_width = int(screen_width * 0.6)\n",
        "    cart_location = get_cart_location(env, screen_width)\n",
        "    if cart_location < view_width // 2:\n",
        "        slice_range = slice(view_width)\n",
        "    elif cart_location > (screen_width - view_width // 2):\n",
        "        slice_range = slice(-view_width, None)\n",
        "    else:\n",
        "        slice_range = slice(cart_location - view_width // 2,\n",
        "                            cart_location + view_width // 2)\n",
        "    # Strip off the edges, so that we have a square image centered on a cart\n",
        "    screen = screen[:, :, slice_range]\n",
        "    # Convert to float, rescale, convert to torch tensor\n",
        "    # (this doesn't require a copy)\n",
        "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "    screen = torch.from_numpy(screen)\n",
        "    # Resize, and add a batch dimension (BCHW)\n",
        "    return resize(screen).unsqueeze(0).to(device)\n",
        "\n",
        "def plot_durations():\n",
        "\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "\n",
        "    # Take 100 episode averages and plot them too\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))"
      ],
      "metadata": {
        "id": "MHsCDFHLx8Z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DQN"
      ],
      "metadata": {
        "id": "CJ566PnTyEDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, gamma, n_actions, screen_height, screen_width,\n",
        "                 batch_size, eps_start = 0.9, eps_end = 0.05,\n",
        "                eps_decay = 200, mem_size = 1000000):\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.eps_start = eps_start\n",
        "        self.eps_end = eps_end\n",
        "        self.eps_decay = eps_decay\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "        self.policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
        "        self.target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "\n",
        "        self.optimizer = optim.RMSprop(self.policy_net.parameters())\n",
        "        self.memory = ReplayMemory(mem_size)\n",
        "\n",
        "        self.steps_done = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        sample = random.random()\n",
        "        eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * \\\n",
        "            math.exp(-1. * self.steps_done / self.eps_decay)\n",
        "        self.steps_done += 1\n",
        "        if sample > eps_threshold:\n",
        "            with torch.no_grad():\n",
        "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
        "        else:\n",
        "            return torch.tensor([[random.randrange(self.n_actions)]], device=device, dtype=torch.long)\n",
        "\n",
        "    def store_transition(self, state, action, next_state, reward):\n",
        "        self.memory.push(state, action, next_state, reward)\n",
        "\n",
        "    def update_target(self):\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "\n",
        "    def learn(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "        transitions = self.memory.sample(self.batch_size)\n",
        "        batch = Transition(*zip(*transitions))\n",
        "\n",
        "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                              batch.next_state)), device=device, dtype=torch.bool)\n",
        "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                    if s is not None])\n",
        "        state_batch = torch.cat(batch.state)\n",
        "        action_batch = torch.cat(batch.action)\n",
        "        reward_batch = torch.Tensor(batch.reward)\n",
        "\n",
        "\n",
        "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "        next_state_values = torch.zeros(self.batch_size, device=device)\n",
        "        next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n",
        "        # Compute the expected Q values\n",
        "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
        "\n",
        "        # Compute Huber loss\n",
        "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "        # Optimize the model\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        for param in self.policy_net.parameters():\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "        self.optimizer.step()"
      ],
      "metadata": {
        "id": "VYulR_Zox_ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayMemory(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "Vw0AacsIyMjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, h, w, outputs):\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "\n",
        "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
        "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
        "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
        "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
        "        linear_input_size = convw * convh * 32\n",
        "        self.head = nn.Linear(linear_input_size, outputs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        return self.head(x.view(x.size(0), -1))"
      ],
      "metadata": {
        "id": "xJoDjZTbyOuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Main"
      ],
      "metadata": {
        "id": "yFM0_lpkyCU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    env = gym.make('CartPole-v0').unwrapped\n",
        "\n",
        "    batch_size = 128\n",
        "    gamma = 0.999\n",
        "    eps_start = 0.9\n",
        "    eps_end = 0.05\n",
        "    eps_decay = 200\n",
        "    target_update = 10\n",
        "\n",
        "    n_actions = env.action_space.n\n",
        "    env.reset()\n",
        "    init_screen = get_screen(env)\n",
        "    _, _, screen_height, screen_width = init_screen.shape\n",
        "    episode_durations = []\n",
        "\n",
        "\n",
        "    agent = Agent(gamma = gamma, n_actions = n_actions, screen_height = screen_height,\n",
        "                  screen_width = screen_width, batch_size = batch_size)\n",
        "\n",
        "    n_games = 500\n",
        "    scores = []\n",
        "    eps_history = []\n",
        "\n",
        "    for i in range(n_games):\n",
        "        done = False\n",
        "        length = 0\n",
        "        state = env.reset()\n",
        "        last_screen = get_screen(env)\n",
        "        current_screen = get_screen(env)\n",
        "        state = current_screen - last_screen\n",
        "        # env.render()\n",
        "        while not done:\n",
        "            action = agent.select_action(state)\n",
        "            _, reward, done, _ = env.step(action.item())\n",
        "            last_screen = current_screen\n",
        "            current_screen = get_screen(env)\n",
        "\n",
        "            next_state = current_screen - last_screen if not done else None\n",
        "            agent.store_transition(state, action, next_state, reward)\n",
        "            state = next_state\n",
        "\n",
        "            agent.learn()\n",
        "            length += 1\n",
        "        episode_durations.append(length)\n",
        "        # plot_durations()\n",
        "\n",
        "        if i % target_update == 0:\n",
        "            agent.update_target()\n",
        "\n",
        "\n",
        "    #imagine pretty plots\n",
        "    print('done')\n",
        "    env.close()    "
      ],
      "metadata": {
        "id": "JzpVXL1Sx784"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}